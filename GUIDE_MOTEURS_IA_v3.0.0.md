# ü§ñ Guide des Moteurs IA - RolePlay AI v3.0.0

## üìã Nouveaut√©s de la v3.0.0

Cette version introduit un **syst√®me de moteurs IA modulaire** qui vous permet de choisir comment les personnages g√©n√®rent leurs r√©ponses !

### ‚ú® Fonctionnalit√©s ajout√©es

- ‚úÖ **5 moteurs IA** au choix : Groq, Gemini Nano, llama.cpp, Together AI, SmartLocalAI
- ‚úÖ **S√©lection dans les param√®tres** : Changez de moteur √† tout moment
- ‚úÖ **Fallbacks automatiques** : Si un moteur √©choue, l'app bascule automatiquement
- ‚úÖ **Support llama.cpp** : Utilisez des mod√®les locaux GGUF (Phi-3, Gemma, etc.)
- ‚úÖ **Gemini Nano** : IA Google int√©gr√©e (Android 14+)
- ‚úÖ **Architecture NDK** : Code natif C++ pour performances optimales

---

## üöÄ Moteurs Disponibles

### 1. **Groq API** ‚òÅÔ∏è (Par d√©faut)
- **Type** : Cloud (n√©cessite Internet)
- **Qualit√©** : ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellente
- **Vitesse** : ‚ö°‚ö°‚ö°‚ö°‚ö° Ultra-rapide (1-2s)
- **Gratuit** : Oui (avec cl√©s API)
- **NSFW** : ‚úÖ Support√©

**Avantages** :
- R√©ponses quasi-instantan√©es gr√¢ce aux LPU Groq
- Mod√®les puissants (Llama 3.3 70B, Mixtral)
- Rotation automatique des cl√©s API

**Configuration** :
1. Obtenir une cl√© gratuite sur [console.groq.com](https://console.groq.com)
2. Param√®tres > Configuration Admin > Ajouter cl√© Groq

---

### 2. **Gemini Nano** üì± (Recommand√© si compatible)
- **Type** : Local (on-device)
- **Qualit√©** : ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellente
- **Vitesse** : ‚ö°‚ö°‚ö°‚ö°‚ö° Tr√®s rapide (2-5s)
- **Gratuit** : Oui (100% gratuit)
- **NSFW** : ‚úÖ Support√©

**Pr√©requis** :
- Android 14+ (API 34+)
- Appareil compatible : Pixel 8+, Samsung S24+, OnePlus 12+
- Google Play Services √† jour

**Avantages** :
- Aucune connexion Internet requise
- Qualit√© √©quivalente √† GPT-3.5
- Privacy totale (tout reste sur votre t√©l√©phone)
- Gratuit et illimit√©

**Activation** :
1. V√©rifier que votre appareil est sous Android 14+
2. Param√®tres > Moteur IA > S√©lectionner "Gemini Nano (Local)"
3. Si erreur : votre appareil ne supporte pas Gemini Nano

---

### 3. **llama.cpp** üì± (Pour experts)
- **Type** : Local (mod√®les GGUF)
- **Qualit√©** : ‚≠ê‚≠ê‚≠ê‚≠ê √† ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (selon mod√®le)
- **Vitesse** : ‚ö°‚ö°‚ö° √† ‚ö°‚ö°‚ö°‚ö° (3-10s selon mod√®le)
- **Gratuit** : Oui (mod√®les √† t√©l√©charger)
- **NSFW** : ‚úÖ Support√©

**Mod√®les recommand√©s** :

#### Phi-3 Mini (2.2 GB) - ‚≠ê Recommand√©
- **RAM requise** : 6 GB+
- **Qualit√©** : Excellente (√©quivalent GPT-3.5)
- **T√©l√©chargement** : [HuggingFace](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf)

#### Gemma 2B (1.5 GB) - L√©ger
- **RAM requise** : 4 GB+
- **Qualit√©** : Tr√®s bonne
- **T√©l√©chargement** : [HuggingFace](https://huggingface.co/google/gemma-2b-it-gguf/resolve/main/gemma-2b-it-q4_k_m.gguf)

#### TinyLlama (630 MB) - Ultra-l√©ger
- **RAM requise** : 2 GB+
- **Qualit√©** : Correcte
- **T√©l√©chargement** : [HuggingFace](https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.q4_k_m.gguf)

**Installation** :
1. T√©l√©charger un mod√®le `.gguf`
2. Placer dans `/sdcard/Android/data/com.roleplayai.chatbot/files/models/`
3. Param√®tres > Moteur IA > S√©lectionner "llama.cpp (Local)"
4. Param√®tres > Mod√®le llama.cpp > Choisir votre mod√®le

**Note** : N√©cessite la compilation NDK (voir section d√©veloppeur)

---

### 4. **Together AI** ‚òÅÔ∏è (Fallback)
- **Type** : Cloud (n√©cessite Internet)
- **Qualit√©** : ‚≠ê‚≠ê‚≠ê‚≠ê Bonne
- **Vitesse** : ‚ö°‚ö°‚ö° Correcte (5-10s)
- **Gratuit** : Oui (API gratuite)
- **NSFW** : ‚úÖ Support√©

**Avantages** :
- Aucune configuration requise
- Utilis√© automatiquement en fallback
- Gratuit sans limite

---

### 5. **SmartLocalAI** üì± (Fallback ultime)
- **Type** : Local (templates)
- **Qualit√©** : ‚≠ê‚≠ê‚≠ê Acceptable
- **Vitesse** : ‚ö°‚ö°‚ö°‚ö°‚ö° Instantan√©
- **Gratuit** : Oui
- **NSFW** : ‚úÖ Support√©

**Caract√©ristiques** :
- R√©ponses bas√©es sur templates intelligents
- Ne n√©cessite aucun mod√®le
- Toujours disponible (ne peut jamais √©chouer)
- Utilis√© en dernier recours

---

## ‚öôÔ∏è Configuration

### Changer de moteur IA

1. Ouvrir **Param√®tres**
2. Section **"ü§ñ Moteur d'Intelligence Artificielle"**
3. Cliquer sur **"Moteur actuel"**
4. S√©lectionner le moteur souhait√©
5. C'est tout ! Le changement est imm√©diat

### Activer/D√©sactiver les Fallbacks

Par d√©faut, les fallbacks sont **activ√©s**. Si le moteur principal √©choue, l'app basculera automatiquement vers un autre moteur.

**Pour d√©sactiver** :
1. Param√®tres > Moteur IA
2. D√©sactiver **"Fallbacks automatiques"**
3. L'app utilisera uniquement le moteur s√©lectionn√©

**Cascade de fallbacks** :
- **Groq** ‚Üí Together AI ‚Üí Gemini Nano ‚Üí llama.cpp ‚Üí SmartLocalAI
- **Gemini Nano** ‚Üí llama.cpp ‚Üí Together AI ‚Üí SmartLocalAI
- **llama.cpp** ‚Üí Gemini Nano ‚Üí Together AI ‚Üí SmartLocalAI

---

## üìä Comparaison des Moteurs

| Moteur | Type | Internet | Qualit√© | Vitesse | Setup | NSFW |
|--------|------|----------|---------|---------|-------|------|
| **Groq API** | Cloud | ‚úÖ Oui | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö°‚ö° | API Key | ‚úÖ |
| **Gemini Nano** | Local | ‚ùå Non | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö°‚ö° | Android 14+ | ‚úÖ |
| **llama.cpp** | Local | ‚ùå Non | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö° | Mod√®le GGUF | ‚úÖ |
| **Together AI** | Cloud | ‚úÖ Oui | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö° | Aucun | ‚úÖ |
| **SmartLocalAI** | Local | ‚ùå Non | ‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö°‚ö° | Aucun | ‚úÖ |

---

## üéØ Recommandations par Appareil

### Flagship r√©cent (Android 14+, 8GB+ RAM)
‚û°Ô∏è **Gemini Nano** - Meilleur rapport qualit√©/vitesse, 100% gratuit et priv√©

### Flagship (Android 13-, 8GB+ RAM)
‚û°Ô∏è **Groq API** - Ultra-rapide, excellente qualit√© (n√©cessite cl√©s API)

### Milieu de gamme (4-6GB RAM)
‚û°Ô∏è **Together AI + Fallback SmartLocalAI** - Pas de configuration, fonctionne partout

### Bas de gamme (2-4GB RAM)
‚û°Ô∏è **SmartLocalAI** - L√©ger, rapide, toujours disponible

### Pour experts avec mod√®les locaux
‚û°Ô∏è **llama.cpp (Phi-3 ou Gemma)** - Privacy maximale, offline complet

---

## üõ†Ô∏è Pour D√©veloppeurs

### Architecture Technique

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         ChatViewModel               ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ    ‚îÇ   AIOrchestrator    ‚îÇ          ‚îÇ
‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ              ‚îÇ                      ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ    ‚îÇ  S√©lection moteur + Config ‚îÇ   ‚îÇ
‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ              ‚îÇ                      ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ    ‚îÇ  G√©n√©ration avec fallbacks ‚îÇ   ‚îÇ
‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ              ‚îÇ                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ              ‚îÇ                      ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ    ‚îÇ  Moteurs IA       ‚îÇ            ‚îÇ
‚îÇ    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§            ‚îÇ
‚îÇ    ‚îÇ GroqAIEngine      ‚îÇ            ‚îÇ
‚îÇ    ‚îÇ GeminiNanoEngine  ‚îÇ            ‚îÇ
‚îÇ    ‚îÇ LlamaCppEngine    ‚îÇ            ‚îÇ
‚îÇ    ‚îÇ TogetherAIEngine  ‚îÇ            ‚îÇ
‚îÇ    ‚îÇ SmartLocalAI      ‚îÇ            ‚îÇ
‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Fichiers Cr√©√©s/Modifi√©s

**Nouveaux fichiers** :
- `GeminiNanoEngine.kt` - Int√©gration Gemini Nano
- `LlamaCppEngine.kt` - Interface llama.cpp avec JNI
- `AIOrchestrator.kt` - Routeur intelligent des moteurs
- `llama-android.cpp` - Code natif JNI pour llama.cpp
- `CMakeLists.txt` - Configuration CMake pour NDK

**Fichiers modifi√©s** :
- `PreferencesManager.kt` - Ajout pr√©f√©rences moteur IA
- `SettingsViewModel.kt` - Ajout flows et setters
- `SettingsScreen.kt` - UI s√©lection moteur
- `ChatViewModel.kt` - Utilisation AIOrchestrator
- `build.gradle.kts` - Activation NDK

### Build NDK

Pour compiler le code natif llama.cpp :

```bash
# Installer NDK
sdkmanager --install "ndk;26.1.10909125"

# Build
./gradlew assembleDebug

# Le .so sera g√©n√©r√© dans :
# app/build/intermediates/cmake/debug/obj/arm64-v8a/libllama-android.so
```

**Note** : La biblioth√®que llama.cpp compl√®te n'est pas encore int√©gr√©e. Le wrapper JNI est pr√™t mais retourne un fallback. Pour l'int√©gration compl√®te :

1. Cloner llama.cpp : `git clone https://github.com/ggerganov/llama.cpp app/src/main/cpp/llama.cpp`
2. D√©commenter les sections TODO dans `CMakeLists.txt`
3. Impl√©menter les fonctions dans `llama-android.cpp`

---

## üêõ D√©pannage

### Gemini Nano : "Non disponible"
- V√©rifier Android 14+ : Param√®tres > √Ä propos > Version Android
- V√©rifier Google Play Services √† jour
- Votre appareil ne supporte peut-√™tre pas Gemini Nano

### llama.cpp : "Mod√®le non trouv√©"
- V√©rifier que le fichier `.gguf` est dans `/sdcard/Android/data/com.roleplayai.chatbot/files/models/`
- V√©rifier les permissions de stockage
- R√©essayer la s√©lection dans Param√®tres

### llama.cpp : "Erreur de chargement"
- RAM insuffisante (fermer d'autres apps)
- Mod√®le trop gros pour votre appareil
- Essayer un mod√®le plus petit (TinyLlama)

### Groq : "Cl√© API manquante"
- Ajouter une cl√© dans Param√®tres > Configuration Admin
- V√©rifier que la cl√© est valide sur console.groq.com

### R√©ponses lentes
- Changer de moteur (Gemini Nano ou Groq sont les plus rapides)
- Activer fallbacks automatiques
- V√©rifier la connexion Internet (pour moteurs cloud)

---

## üìà Logs de Debug

L'app g√©n√®re des logs d√©taill√©s pour chaque g√©n√©ration :

```
ChatViewModel: ü§ñ Moteur s√©lectionn√©: GEMINI_NANO
AIOrchestrator: ===== AI Orchestrator =====
AIOrchestrator: Moteur principal: Gemini Nano (Local)
GeminiNanoEngine: ‚úÖ Gemini Nano initialis√©
GeminiNanoEngine: ‚úÖ R√©ponse Gemini Nano: *rougit*...
ChatViewModel: ‚úÖ R√©ponse g√©n√©r√©e par GEMINI_NANO en 3421ms
```

Pour voir les logs :
```bash
adb logcat | grep -E "ChatViewModel|AIOrchestrator|GeminiNano|LlamaCpp"
```

---

## üéâ Conclusion

Cette version 3.0.0 transforme RolePlay AI en une **plateforme IA modulaire** o√π vous avez le contr√¥le total sur :
- Le moteur utilis√©
- La privacy (local vs cloud)
- Le co√ªt (gratuit vs API)
- Les performances

**Recommandation g√©n√©rale** : Commencer avec **Groq API** (excellent par d√©faut), puis tester **Gemini Nano** si vous avez Android 14+. Pour une privacy maximale, utiliser **llama.cpp** avec Phi-3.

Enjoy ! üöÄ

---

**D√©velopp√© par l'√©quipe RolePlay AI**
*Version 3.0.0 - D√©cembre 2024*
