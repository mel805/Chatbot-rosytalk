# Configuration de l'API IA - RolePlay AI

## ü§ñ Options d'IA Disponibles

RolePlay AI supporte plusieurs backends d'IA :

1. **HuggingFace Inference API** (Par d√©faut - Gratuit)
2. **API Locale** (LM Studio, Ollama, etc.)
3. **OpenAI Compatible APIs**

## 1Ô∏è‚É£ HuggingFace (Recommand√© pour d√©buter)

### Avantages
- ‚úÖ Gratuit
- ‚úÖ Aucune installation requise
- ‚úÖ Fonctionne imm√©diatement
- ‚úÖ Mod√®le puissant (Mistral-7B)

### Limitations
- ‚ö†Ô∏è N√©cessite Internet
- ‚ö†Ô∏è Rate limiting possible
- ‚ö†Ô∏è Peut √™tre lent aux heures de pointe

### Configuration

#### Sans Token (Limite de d√©bit)

L'application fonctionne directement sans configuration !

#### Avec Token (Recommand√©)

Pour de meilleures performances :

1. **Cr√©er un compte HuggingFace**
   - Aller sur https://huggingface.co/join
   - Cr√©er un compte gratuit

2. **Obtenir un Access Token**
   - Aller dans Settings > Access Tokens
   - Cr√©er un nouveau token (Read only suffit)
   - Copier le token

3. **Configurer dans l'application**

Modifier `AIEngine.kt` ligne ~19 :

```kotlin
private var apiKey = "hf_xxxxxxxxxxxxxxxxxxxxx" // Votre token ici
```

Ou cr√©er un √©cran de param√®tres dans l'app (√† venir).

### Mod√®les Disponibles

**Actuellement utilis√©** : `mistralai/Mistral-7B-Instruct-v0.2`

**Alternatives possibles** :
```kotlin
// Dans AIEngine.kt, changer apiEndpoint

// Mistral (actuel)
"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2"

// Llama 2
"https://api-inference.huggingface.co/models/meta-llama/Llama-2-7b-chat-hf"

// Falcon
"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct"

// Zephyr
"https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta"
```

### Exemple d'Int√©gration

```kotlin
val aiEngine = AIEngine(context)

// Utiliser le token HuggingFace
aiEngine.setAPIKey("hf_xxxxxxxxxxxxx")

// Optionnel : Changer de mod√®le
aiEngine.setAPIEndpoint(
    "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2"
)
```

## 2Ô∏è‚É£ API Locale (Pour utilisateurs avanc√©s)

### Avantages
- ‚úÖ Pas de limite de d√©bit
- ‚úÖ Confidentialit√© totale
- ‚úÖ Pas de connexion Internet n√©cessaire
- ‚úÖ Personnalisation compl√®te

### Limitations
- ‚ö†Ô∏è N√©cessite un serveur local
- ‚ö†Ô∏è Configuration technique
- ‚ö†Ô∏è Ressources machine importantes

### Option A : LM Studio

**LM Studio** est un client desktop pour ex√©cuter des LLM localement.

#### Installation

1. **T√©l√©charger LM Studio**
   - Site : https://lmstudio.ai/
   - T√©l√©charger pour Windows/Mac/Linux
   - Installer l'application

2. **T√©l√©charger un Mod√®le**
   - Ouvrir LM Studio
   - Aller dans l'onglet "Discover"
   - Chercher "Mistral-7B-Instruct"
   - T√©l√©charger une version GGUF (ex: Q4_K_M)
   - Attendre le t√©l√©chargement (4-8 GB)

3. **D√©marrer le Serveur**
   - Aller dans l'onglet "Local Server"
   - S√©lectionner le mod√®le t√©l√©charg√©
   - Cliquer sur "Start Server"
   - Le serveur d√©marre sur `http://localhost:1234`

4. **Configurer l'Application Android**

   **Important** : Votre t√©l√©phone doit √™tre sur le m√™me r√©seau WiFi que votre PC.

   Trouver l'IP de votre PC :
   ```bash
   # Windows
   ipconfig
   
   # Mac/Linux
   ifconfig
   ```

   Modifier `ChatViewModel.kt` ou `AIEngine.kt` :

   ```kotlin
   val aiEngine = AIEngine(application)
   
   // Remplacer localhost par l'IP de votre PC
   aiEngine.setUseLocalAPI(true, "http://192.168.1.XXX:1234/v1/chat/completions")
   ```

### Option B : Ollama

**Ollama** est une alternative l√©g√®re √† LM Studio.

#### Installation

1. **Installer Ollama**
   ```bash
   # Mac/Linux
   curl -fsSL https://ollama.com/install.sh | sh
   
   # Ou t√©l√©charger depuis https://ollama.com/download
   ```

2. **T√©l√©charger un Mod√®le**
   ```bash
   ollama pull mistral
   # ou
   ollama pull llama2
   ```

3. **D√©marrer le Serveur**
   ```bash
   ollama serve
   ```
   
   Le serveur d√©marre sur `http://localhost:11434`

4. **Configurer l'Application**
   ```kotlin
   aiEngine.setUseLocalAPI(true, "http://192.168.1.XXX:11434/v1/chat/completions")
   ```

### Option C : Text Generation WebUI

#### Installation

1. **Cloner le Repository**
   ```bash
   git clone https://github.com/oobabooga/text-generation-webui
   cd text-generation-webui
   ```

2. **Installer les D√©pendances**
   ```bash
   # Suivre les instructions du README
   pip install -r requirements.txt
   ```

3. **T√©l√©charger un Mod√®le**
   - Via l'interface web
   - Ou manuellement dans le dossier `models/`

4. **Lancer avec API**
   ```bash
   python server.py --api --listen
   ```

5. **Configurer l'Application**
   ```kotlin
   aiEngine.setUseLocalAPI(true, "http://192.168.1.XXX:5000/v1/chat/completions")
   ```

## 3Ô∏è‚É£ APIs Compatibles OpenAI

### OpenAI API (Payant)

Si vous avez une cl√© API OpenAI :

```kotlin
val aiEngine = AIEngine(context)

// Utiliser OpenAI
aiEngine.setAPIEndpoint("https://api.openai.com/v1/chat/completions")
aiEngine.setAPIKey("sk-xxxxxxxxxxxxx")

// Note: N√©cessite une modification du code pour le format OpenAI
```

**‚ö†Ô∏è Attention** : OpenAI est payant et peut co√ªter cher avec une utilisation intensive.

### Autres APIs

**APIs compatibles OpenAI** :
- Groq (gratuit, tr√®s rapide)
- Together AI
- Anyscale
- Replicate

**Exemple avec Groq** :

1. Cr√©er un compte sur https://console.groq.com/
2. Obtenir une cl√© API
3. Configurer :

```kotlin
aiEngine.setAPIEndpoint("https://api.groq.com/openai/v1/chat/completions")
aiEngine.setAPIKey("gsk_xxxxxxxxxxxxx")
```

## üîß Configuration Avanc√©e

### Modifier les Param√®tres de G√©n√©ration

Dans `AIEngine.kt`, ajuster les param√®tres :

```kotlin
data class HFParameters(
    @SerializedName("max_new_tokens")
    val maxNewTokens: Int = 500,  // Longueur max de r√©ponse
    
    @SerializedName("temperature")
    val temperature: Float = 0.9,  // Cr√©ativit√© (0.1-1.0)
    
    @SerializedName("top_p")
    val topP: Float = 0.95,  // Diversit√©
    
    @SerializedName("return_full_text")
    val returnFullText: Boolean = false
)
```

**Param√®tres expliqu√©s** :

- **max_new_tokens** (100-1000)
  - Plus haut = r√©ponses plus longues
  - Recommand√© : 500

- **temperature** (0.1-2.0)
  - Plus bas = plus pr√©visible
  - Plus haut = plus cr√©atif
  - Recommand√© : 0.8-1.0

- **top_p** (0.1-1.0)
  - Contr√¥le la diversit√©
  - Recommand√© : 0.9-0.95

### Personnaliser le System Prompt

Dans `AIEngine.kt`, fonction `buildPrompt()` :

```kotlin
private fun buildPrompt(character: Character, messages: List<Message>): String {
    val systemPrompt = """
        Tu es ${character.name}.
        Description: ${character.description}
        Personnalit√©: ${character.personality}
        Sc√©nario: ${character.scenario}
        
        Instructions suppl√©mentaires:
        - Rester dans le personnage √† tout moment
        - √ätre naturel et engageant
        - Adapter le ton √† la personnalit√©
        - Utiliser *action* pour les actions physiques
        - √ätre coh√©rent avec l'historique
    """.trimIndent()
    
    // ... rest of the code
}
```

### G√©rer le Contexte

**Nombre de messages dans le contexte** :

```kotlin
val conversationHistory = messages.takeLast(10)  // Ajuster ce nombre
```

- Plus petit (5) = M√©moire courte, plus rapide
- Plus grand (20) = Meilleure m√©moire, plus lent

### Timeout et Retry

Configurer dans `AIEngine.kt` :

```kotlin
private val client = OkHttpClient.Builder()
    .connectTimeout(30, TimeUnit.SECONDS)  // Connexion
    .readTimeout(60, TimeUnit.SECONDS)     // Lecture
    .writeTimeout(30, TimeUnit.SECONDS)    // √âcriture
    .build()
```

## üìä Comparaison des Options

| Option | Co√ªt | Vitesse | Qualit√© | Confidentialit√© | Difficult√© |
|--------|------|---------|---------|-----------------|------------|
| HuggingFace Free | Gratuit | Moyenne | Bonne | Moyenne | Facile ‚úÖ |
| HuggingFace + Token | Gratuit | Bonne | Bonne | Moyenne | Facile ‚úÖ |
| LM Studio | Gratuit | Bonne* | Excellente | Totale | Moyenne |
| Ollama | Gratuit | Bonne* | Excellente | Totale | Moyenne |
| OpenAI API | Payant | Tr√®s rapide | Excellente | Faible | Facile |
| Groq | Gratuit | Tr√®s rapide | Bonne | Faible | Facile |

*D√©pend de votre mat√©riel

## üõ†Ô∏è D√©pannage

### L'IA ne r√©pond pas

1. **V√©rifier la connexion**
   ```bash
   # Test HuggingFace
   curl https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2
   
   # Test API Locale
   curl http://localhost:1234/v1/models
   ```

2. **V√©rifier les logs**
   - Dans Android Studio : Logcat
   - Filtrer par "AIEngine"

3. **Tester l'endpoint directement**
   ```bash
   curl -X POST https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 \
     -H "Content-Type: application/json" \
     -d '{"inputs":"Hello, how are you?"}'
   ```

### Erreurs Fr√©quentes

**"Model is loading"** (HuggingFace)
- Le mod√®le se charge (premi√®re requ√™te)
- Attendre 30 secondes et r√©essayer

**"Rate limit exceeded"** (HuggingFace)
- Trop de requ√™tes
- Attendre quelques minutes
- Utiliser un token API

**"Connection refused"** (Local)
- V√©rifier que le serveur est d√©marr√©
- V√©rifier l'IP et le port
- V√©rifier le pare-feu

**"Timeout"**
- Augmenter le timeout dans le code
- V√©rifier la connexion r√©seau
- Le mod√®le peut √™tre trop lent

## üéØ Recommandations

### Pour D√©buter
‚Üí **HuggingFace gratuit** : Aucune configuration, fonctionne imm√©diatement

### Pour Usage R√©gulier
‚Üí **HuggingFace + Token** : Meilleures performances, toujours gratuit

### Pour Confidentialit√©
‚Üí **LM Studio** : Local, priv√©, pas de connexion Internet

### Pour Performance
‚Üí **Groq API** : Ultra rapide, gratuit avec limites

### Pour Production
‚Üí **OpenAI API** ou **serveur d√©di√©** : Fiable et puissant

## üìö Ressources

- [HuggingFace Documentation](https://huggingface.co/docs/api-inference)
- [LM Studio](https://lmstudio.ai/)
- [Ollama](https://ollama.com/)
- [Groq](https://console.groq.com/)
- [Text Generation WebUI](https://github.com/oobabooga/text-generation-webui)

---

**Note** : La configuration par d√©faut (HuggingFace gratuit) fonctionne imm√©diatement sans aucune modification !
