#include <jni.h>
#include <string>
#include <android/log.h>
#include <vector>

#ifdef LLAMA_CPP_AVAILABLE
#include "llama.h"
#endif

#define LOG_TAG "llama-android"
#define LOGI(...) __android_log_print(ANDROID_LOG_INFO, LOG_TAG, __VA_ARGS__)
#define LOGE(...) __android_log_print(ANDROID_LOG_ERROR, LOG_TAG, __VA_ARGS__)

// Structure pour stocker le contexte du mod√®le
struct ModelContext {
#ifdef LLAMA_CPP_AVAILABLE
    llama_model* model;
    llama_context* ctx;
#else
    void* dummy;  // Pour √©viter une structure vide
#endif
    std::string model_path;
    bool loaded;
    
    ModelContext() : loaded(false) {
#ifdef LLAMA_CPP_AVAILABLE
        model = nullptr;
        ctx = nullptr;
#else
        dummy = nullptr;
#endif
    }
};

extern "C" {

/**
 * Charge un mod√®le GGUF
 */
JNIEXPORT jlong JNICALL
Java_com_roleplayai_chatbot_data_ai_LlamaCppEngine_loadModel(
    JNIEnv* env, jclass clazz,
    jstring modelPath, jint nThreads, jint nCtx
) {
    const char* path_cstr = env->GetStringUTFChars(modelPath, nullptr);
    std::string path(path_cstr);
    env->ReleaseStringUTFChars(modelPath, path_cstr);
    
    LOGI("üöÄ Chargement mod√®le: %s", path.c_str());
    
    ModelContext* context = new ModelContext();
    context->model_path = path;
    
#ifdef LLAMA_CPP_AVAILABLE
    // Initialiser llama.cpp
    llama_backend_init(false);
    
    // Param√®tres du mod√®le
    llama_model_params model_params = llama_model_default_params();
    
    // Charger le mod√®le
    context->model = llama_load_model_from_file(path.c_str(), model_params);
    if (!context->model) {
        LOGE("‚ùå √âchec chargement mod√®le: %s", path.c_str());
        delete context;
        return 0;
    }
    
    // Param√®tres du contexte
    llama_context_params ctx_params = llama_context_default_params();
    ctx_params.n_ctx = nCtx;
    ctx_params.n_threads = nThreads;
    ctx_params.n_threads_batch = nThreads;
    
    // Cr√©er le contexte
    context->ctx = llama_new_context_with_model(context->model, ctx_params);
    if (!context->ctx) {
        LOGE("‚ùå √âchec cr√©ation contexte");
        llama_free_model(context->model);
        delete context;
        return 0;
    }
    
    context->loaded = true;
    LOGI("‚úÖ Mod√®le charg√© avec succ√®s");
    
#else
    LOGI("‚ö†Ô∏è llama.cpp non compil√© - mode fallback");
    context->loaded = false;
#endif
    
    return reinterpret_cast<jlong>(context);
}

/**
 * G√©n√®re du texte
 */
JNIEXPORT jstring JNICALL
Java_com_roleplayai_chatbot_data_ai_LlamaCppEngine_generate(
    JNIEnv* env, jclass clazz,
    jlong contextPtr, jstring prompt,
    jint maxTokens, jfloat temperature, jfloat topP, jint topK, jfloat repeatPenalty
) {
    ModelContext* context = reinterpret_cast<ModelContext*>(contextPtr);
    
    if (!context || !context->loaded) {
        LOGE("‚ùå Contexte invalide ou mod√®le non charg√©");
        return env->NewStringUTF("*sourit* Le moteur llama.cpp n'est pas encore configur√©. Utilisez un autre moteur IA.");
    }
    
    const char* prompt_cstr = env->GetStringUTFChars(prompt, nullptr);
    std::string prompt_str(prompt_cstr);
    env->ReleaseStringUTFChars(prompt, prompt_cstr);
    
    LOGI("üìù G√©n√©ration avec prompt: %s...", prompt_str.substr(0, 50).c_str());
    
#ifdef LLAMA_CPP_AVAILABLE
    // Tokenize le prompt
    std::vector<llama_token> tokens_list;
    tokens_list.resize(prompt_str.size() + 1);
    int n_tokens = llama_tokenize(
        context->model,
        prompt_str.c_str(),
        prompt_str.size(),
        tokens_list.data(),
        tokens_list.size(),
        true,  // add_bos
        false  // special
    );
    tokens_list.resize(n_tokens);
    
    LOGI("üî¢ Tokens: %d", n_tokens);
    
    // √âvaluer le prompt
    llama_batch batch = llama_batch_init(tokens_list.size(), 0, 1);
    for (size_t i = 0; i < tokens_list.size(); i++) {
        llama_batch_add(batch, tokens_list[i], i, {0}, false);
    }
    batch.logits[batch.n_tokens - 1] = true;  // Calculer logits pour le dernier token
    
    if (llama_decode(context->ctx, batch) != 0) {
        LOGE("‚ùå √âchec decode");
        llama_batch_free(batch);
        return env->NewStringUTF("Erreur de g√©n√©ration");
    }
    
    // G√©n√©rer la r√©ponse token par token
    std::string generated_text;
    int n_cur = batch.n_tokens;
    int n_gen = 0;
    
    while (n_gen < maxTokens) {
        // Obtenir les logits
        float* logits = llama_get_logits_ith(context->ctx, batch.n_tokens - 1);
        
        // Sample le prochain token (sampling simple pour commencer)
        int n_vocab = llama_n_vocab(context->model);
        std::vector<llama_token_data> candidates;
        candidates.reserve(n_vocab);
        
        for (llama_token token_id = 0; token_id < n_vocab; token_id++) {
            candidates.push_back({token_id, logits[token_id], 0.0f});
        }
        
        llama_token_data_array candidates_p = {
            candidates.data(),
            candidates.size(),
            false
        };
        
        // Sample avec temperature
        llama_sample_top_k(context->ctx, &candidates_p, topK, 1);
        llama_sample_top_p(context->ctx, &candidates_p, topP, 1);
        llama_sample_temp(context->ctx, &candidates_p, temperature);
        llama_token new_token_id = llama_sample_token(context->ctx, &candidates_p);
        
        // V√©rifier fin de g√©n√©ration
        if (new_token_id == llama_token_eos(context->model)) {
            LOGI("‚úÖ Fin de g√©n√©ration (EOS)");
            break;
        }
        
        // Convertir en texte
        char buf[256];
        int n = llama_token_to_piece(context->model, new_token_id, buf, sizeof(buf));
        if (n > 0) {
            generated_text.append(buf, n);
        }
        
        // Pr√©parer le prochain batch
        llama_batch_clear(batch);
        llama_batch_add(batch, new_token_id, n_cur, {0}, true);
        n_cur++;
        n_gen++;
        
        // √âvaluer le nouveau token
        if (llama_decode(context->ctx, batch) != 0) {
            LOGE("‚ùå √âchec decode token %d", n_gen);
            break;
        }
    }
    
    llama_batch_free(batch);
    
    LOGI("‚úÖ G√©n√©ration termin√©e: %d tokens", n_gen);
    return env->NewStringUTF(generated_text.c_str());
    
#else
    LOGI("‚ö†Ô∏è FALLBACK MODE: llama.cpp pas encore compil√©");
    std::string generated_text = "*sourit* Bonjour ! Le moteur llama.cpp sera disponible apr√®s compilation du NDK avec le mod√®le GGUF.";
    return env->NewStringUTF(generated_text.c_str());
#endif
}

/**
 * Lib√®re le mod√®le
 */
JNIEXPORT void JNICALL
Java_com_roleplayai_chatbot_data_ai_LlamaCppEngine_freeModel(
    JNIEnv* env, jclass clazz, jlong contextPtr
) {
    ModelContext* context = reinterpret_cast<ModelContext*>(contextPtr);
    if (!context) return;
    
#ifdef LLAMA_CPP_AVAILABLE
    if (context->ctx) {
        llama_free(context->ctx);
        context->ctx = nullptr;
    }
    if (context->model) {
        llama_free_model(context->model);
        context->model = nullptr;
    }
    llama_backend_free();
#endif
    
    delete context;
    LOGI("‚úÖ Mod√®le lib√©r√©");
}

/**
 * V√©rifie si le mod√®le est charg√©
 */
JNIEXPORT jboolean JNICALL
Java_com_roleplayai_chatbot_data_ai_LlamaCppEngine_isModelLoaded(
    JNIEnv* env, jclass clazz, jlong contextPtr
) {
    ModelContext* context = reinterpret_cast<ModelContext*>(contextPtr);
    return (context && context->loaded) ? JNI_TRUE : JNI_FALSE;
}

} // extern "C"
