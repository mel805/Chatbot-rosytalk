package com.roleplayai.chatbot.data.ai

import android.content.Context
import android.util.Log
import com.roleplayai.chatbot.data.model.Character
import com.roleplayai.chatbot.data.model.Message
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.withContext

/**
 * AI Orchestrator - Routeur intelligent des moteurs d'IA
 * 
 * G√®re automatiquement la cascade des moteurs d'IA :
 * 1. Moteur principal (Groq / llama.cpp selon config)
 * 2. Fallbacks automatiques si √©chec
 * 3. llama.cpp en dernier recours (ne peut jamais √©chouer)
 * 
 * Architecture :
 * - Rotation automatique des cl√©s Groq (s√©par√©es par virgules)
 * - Cascade intelligente avec fallbacks
 * - Logs d√©taill√©s pour debug
 * - Support NSFW sur tous les moteurs
 */
class AIOrchestrator(
    private val context: Context
) {
    
    // Gestion simple de la rotation des cl√©s Groq
    private var currentGroqKeyIndex = 0
    private val failedGroqKeys = mutableSetOf<String>()

    // IMPORTANT: conserver une instance llama.cpp pour √©viter de recharger le GGUF √† chaque message
    private val llamaEngine: LlamaCppEngine by lazy { LlamaCppEngine(context) }
    private var llamaEnginePath: String? = null
    
    companion object {
        private const val TAG = "AIOrchestrator"
    }

    private fun getOrConfigureLlamaEngine(modelPath: String?): LlamaCppEngine {
        if (modelPath.isNullOrBlank()) {
            // L'appelant g√®re l'erreur "GGUF non configur√©"
            return llamaEngine
        }
        if (llamaEnginePath != modelPath) {
            llamaEngine.setModelPath(modelPath)
            llamaEnginePath = modelPath
        }
        return llamaEngine
    }
    
    /**
     * Moteurs d'IA disponibles
     */
    enum class AIEngine {
        GROQ,           // API Groq (ultra-rapide, cloud)
        LLAMA_CPP;      // llama.cpp (local, intelligent)
        
        fun getDisplayName(): String = when(this) {
            GROQ -> "Groq API (Cloud)"
            LLAMA_CPP -> "llama.cpp (Local)"
        }
        
        fun getDescription(): String = when(this) {
            GROQ -> "Ultra-rapide (1-2s), excellente qualit√©. N√©cessite cl√© API gratuite. Supporte plusieurs cl√©s s√©par√©es par virgules."
            LLAMA_CPP -> "IA locale intelligente. 100% priv√©, fonctionne hors-ligne. R√©ponses uniques et pertinentes."
        }
        
        fun isLocal(): Boolean = when(this) {
            GROQ -> false
            LLAMA_CPP -> true
        }
        
        fun requiresInternet(): Boolean = !isLocal()
    }
    
    /**
     * Configuration de g√©n√©ration
     */
    data class GenerationConfig(
        val primaryEngine: AIEngine,
        val enableFallbacks: Boolean = true,
        val nsfwMode: Boolean = false,
        val groqApiKey: String? = null,  // Peut contenir plusieurs cl√©s s√©par√©es par virgules
        val groqModelId: String? = null,
        val llamaCppModelPath: String? = null
    )
    
    /**
     * R√©sultat de g√©n√©ration
     */
    data class GenerationResult(
        val response: String,
        val usedEngine: AIEngine,
        val generationTimeMs: Long,
        val hadFallback: Boolean
    )
    
    /**
     * G√©n√®re une r√©ponse avec le moteur configur√© et fallbacks automatiques
     */
    suspend fun generateResponse(
        character: Character,
        messages: List<Message>,
        username: String,
        userGender: String,
        memoryContext: String,
        config: GenerationConfig
    ): GenerationResult = withContext(Dispatchers.IO) {
        
        Log.i(TAG, "===== AI Orchestrator =====")
        Log.d(TAG, "Moteur principal: ${config.primaryEngine.getDisplayName()}")
        Log.d(TAG, "Fallbacks: ${config.enableFallbacks}, NSFW: ${config.nsfwMode}")
        
        val startTime = System.currentTimeMillis()
        
        fun isUsable(engine: AIEngine): Boolean {
            return when (engine) {
                AIEngine.GROQ -> config.groqApiKey?.isNotBlank() == true
                AIEngine.LLAMA_CPP -> !config.llamaCppModelPath.isNullOrBlank()
            }
        }

        var primaryError: Exception? = null

        // Essayer le moteur principal
        try {
            if (!isUsable(config.primaryEngine)) {
                throw Exception(
                    when (config.primaryEngine) {
                        AIEngine.GROQ -> "Aucune cl√© API Groq configur√©e."
                        AIEngine.LLAMA_CPP -> "Aucun mod√®le GGUF s√©lectionn√© pour llama.cpp."
                    }
                )
            }
            val response = generateWithEngine(
                engine = config.primaryEngine,
                character = character,
                messages = messages,
                username = username,
                userGender = userGender,
                memoryContext = memoryContext,
                config = config
            )
            
            val duration = System.currentTimeMillis() - startTime
            Log.i(TAG, "‚úÖ Succ√®s avec ${config.primaryEngine.name} en ${duration}ms")
            
            return@withContext GenerationResult(
                response = response,
                usedEngine = config.primaryEngine,
                generationTimeMs = duration,
                hadFallback = false
            )
            
        } catch (e: Exception) {
            primaryError = e
            Log.w(TAG, "‚ö†Ô∏è √âchec moteur principal (${config.primaryEngine.name}): ${e.message}")
            
            if (!config.enableFallbacks) {
                throw e
            }
        }
        
        // Cascade de fallbacks
        val fallbackEngines = getFallbackCascade(config.primaryEngine)
            .filter { isUsable(it) }
        
        for (fallbackEngine in fallbackEngines) {
            try {
                Log.d(TAG, "üîÑ Tentative fallback: ${fallbackEngine.getDisplayName()}")
                
                val response = generateWithEngine(
                    engine = fallbackEngine,
                    character = character,
                    messages = messages,
                    username = username,
                    userGender = userGender,
                    memoryContext = memoryContext,
                    config = config
                )
                
                val duration = System.currentTimeMillis() - startTime
                Log.i(TAG, "‚úÖ Succ√®s avec fallback ${fallbackEngine.name} en ${duration}ms")
                
                return@withContext GenerationResult(
                    response = response,
                    usedEngine = fallbackEngine,
                    generationTimeMs = duration,
                    hadFallback = true
                )
                
            } catch (e: Exception) {
                Log.w(TAG, "‚ö†Ô∏è √âchec fallback ${fallbackEngine.name}: ${e.message}")
            }
        }

        // Aucun fallback utilisable -> remonter l'erreur primaire (important pour Groq sans GGUF)
        throw primaryError ?: Exception("Aucun moteur IA utilisable (Groq cl√© manquante et/ou GGUF non configur√©).")
    }
    
    /**
     * G√©n√®re avec un moteur sp√©cifique
     */
    private suspend fun generateWithEngine(
        engine: AIEngine,
        character: Character,
        messages: List<Message>,
        username: String,
        userGender: String,
        memoryContext: String,
        config: GenerationConfig
    ): String {
        return when (engine) {
            AIEngine.GROQ -> {
                // Parser les cl√©s (peuvent √™tre s√©par√©es par virgules)
                val keysString = config.groqApiKey ?: ""
                Log.d(TAG, "üì• Cl√©s Groq brutes re√ßues: ${if (keysString.isBlank()) "(vide)" else "'${keysString.take(50)}...'"}")
                
                val apiKeys = keysString.split(",").map { it.trim() }.filter { it.isNotBlank() }
                
                if (apiKeys.isEmpty()) {
                    Log.e(TAG, "‚ùå ERREUR: Aucune cl√© Groq trouv√©e apr√®s parsing!")
                    throw Exception("Aucune cl√© API Groq configur√©e. Ajoutez vos cl√©s dans les param√®tres.")
                }
                
                Log.d(TAG, "üìä ${apiKeys.size} cl√©(s) Groq disponible(s) apr√®s parsing")
                apiKeys.forEachIndexed { i, key ->
                    Log.d(TAG, "   üîë Cl√© ${i + 1}: ${key.take(20)}... (${key.length} caract√®res)")
                }
                
                val modelId = config.groqModelId ?: "llama-3.1-8b-instant"
                
                // Essayer chaque cl√© jusqu'√† ce qu'une fonctionne
                var lastError: Exception? = null
                for (attempt in 0 until apiKeys.size) {
                    val keyIndex = (currentGroqKeyIndex + attempt) % apiKeys.size
                    val apiKey = apiKeys[keyIndex]
                    
                    // Ignorer les cl√©s qui ont d√©j√† √©chou√©
                    if (failedGroqKeys.contains(apiKey)) {
                        Log.d(TAG, "‚è≠Ô∏è Cl√© ${keyIndex + 1} d√©j√† en √©chec, skip")
                        continue
                    }
                    
                    try {
                        Log.d(TAG, "üîë Essai avec cl√© ${keyIndex + 1}/${apiKeys.size}")
                        val groqEngine = GroqAIEngine(apiKey, modelId, config.nsfwMode)
                        val response = groqEngine.generateResponse(character, messages, username, userGender, memoryContext)
                        
                        // Succ√®s ! Mettre √† jour l'index
                        currentGroqKeyIndex = keyIndex
                        Log.i(TAG, "‚úÖ Cl√© ${keyIndex + 1} fonctionne")
                        return response
                        
                    } catch (e: Exception) {
                        Log.w(TAG, "‚ö†Ô∏è Cl√© ${keyIndex + 1} √©choue: ${e.message}")
                        
                        // Si rate limit, marquer comme √©chou√©e et essayer la suivante
                        if (e.message?.contains("429") == true || 
                            e.message?.contains("rate limit", ignoreCase = true) == true ||
                            e.message?.contains("Request too large", ignoreCase = true) == true) {
                            
                            failedGroqKeys.add(apiKey)
                            Log.w(TAG, "üö´ Cl√© ${keyIndex + 1} blacklist√©e (rate limit)")
                        }
                        
                        lastError = e
                    }
                }
                
                // Toutes les cl√©s ont √©chou√©
                throw lastError ?: Exception("Toutes les cl√©s Groq ont √©chou√©")
            }
            
            AIEngine.LLAMA_CPP -> {
                val engineInstance = getOrConfigureLlamaEngine(config.llamaCppModelPath)
                engineInstance.generateResponse(character, messages, username, userGender, memoryContext, config.nsfwMode)
            }
        }
    }
    
    /**
     * D√©termine la cascade de fallbacks selon le moteur principal
     */
    private fun getFallbackCascade(primaryEngine: AIEngine): List<AIEngine> {
        return when (primaryEngine) {
            AIEngine.GROQ -> listOf(AIEngine.LLAMA_CPP)
            AIEngine.LLAMA_CPP -> listOf(AIEngine.GROQ)
        }
    }
    
    /**
     * V√©rifie si un moteur est disponible sur cet appareil
     */
    suspend fun isEngineAvailable(engine: AIEngine, config: GenerationConfig): Boolean {
        return try {
            when (engine) {
                AIEngine.GROQ -> config.groqApiKey?.isNotBlank() == true
                AIEngine.LLAMA_CPP -> {
                    val engineInstance = getOrConfigureLlamaEngine(config.llamaCppModelPath)
                    engineInstance.isAvailable()
                }
            }
        } catch (e: Exception) {
            Log.e(TAG, "Erreur v√©rification disponibilit√© ${engine.name}", e)
            false
        }
    }
}
