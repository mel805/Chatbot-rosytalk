package com.roleplayai.chatbot.data.ai

import android.content.Context
import android.util.Log
import com.roleplayai.chatbot.data.memory.ConversationMemory
import com.roleplayai.chatbot.data.model.Character
import com.roleplayai.chatbot.data.model.Message
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.withContext
import java.io.File

/**
 * Optimized Local LLM - Utilise de VRAIS mod√®les GGUF avec llama.cpp
 * 
 * Ce moteur utilise des mod√®les de langage R√âELS qui tournent localement :
 * - Phi-3 Mini (3.8B, GGUF, ~2GB)
 * - Gemma 2B (GGUF, ~1.5GB)  
 * - TinyLlama (1.1B, GGUF, ~600MB)
 * 
 * Optimisations :
 * - Quantization Q4_K_M pour vitesse/qualit√©
 * - Context window adaptatif
 * - Batch processing
 * - KV cache optimization
 * 
 * Int√®gre ConversationMemory pour contexte long terme.
 */
class OptimizedLocalLLM(
    private val context: Context,
    private val modelPath: String,
    private val nsfwMode: Boolean = false
) {
    
    companion object {
        private const val TAG = "OptimizedLocalLLM"
        
        // Mod√®les recommand√©s (t√©l√©chargeables par l'utilisateur)
        data class RecommendedModel(
            val name: String,
            val filename: String,
            val size: String,
            val url: String,
            val quality: String,
            val speed: String
        )
        
        val RECOMMENDED_MODELS = listOf(
            RecommendedModel(
                name = "Phi-3 Mini 4K (Q4)",
                filename = "Phi-3-mini-4k-instruct-q4.gguf",
                size = "2.2 GB",
                url = "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf",
                quality = "Excellent",
                speed = "Rapide"
            ),
            RecommendedModel(
                name = "Gemma 2B (Q4)",
                filename = "gemma-2b-it-q4_k_m.gguf",
                size = "1.5 GB",
                url = "https://huggingface.co/google/gemma-2b-it-gguf",
                quality = "Tr√®s bon",
                speed = "Tr√®s rapide"
            ),
            RecommendedModel(
                name = "TinyLlama 1.1B (Q4)",
                filename = "tinyllama-1.1b-chat-v1.0.q4_k_m.gguf",
                size = "630 MB",
                url = "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF",
                quality = "Bon",
                speed = "Ultra-rapide"
            )
        )
    }
    
    private var isModelLoaded = false
    private var conversationMemory: ConversationMemory? = null
    
    // Native methods (JNI avec llama.cpp)
    private external fun nativeLoadModel(
        modelPath: String,
        threads: Int,
        contextSize: Int,
        useGPU: Boolean
    ): Boolean
    
    private external fun nativeGenerate(
        prompt: String,
        maxTokens: Int,
        temperature: Float,
        topP: Float,
        topK: Int,
        repeatPenalty: Float,
        stopSequences: Array<String>
    ): String
    
    private external fun nativeUnloadModel(): Boolean
    
    init {
        try {
            System.loadLibrary("llama-android")
            Log.d(TAG, "‚úÖ Biblioth√®que llama.cpp charg√©e")
        } catch (e: UnsatisfiedLinkError) {
            Log.w(TAG, "‚ö†Ô∏è llama.cpp non disponible: ${e.message}")
        }
    }
    
    /**
     * Charge le mod√®le GGUF
     */
    suspend fun loadModel(characterId: String): Boolean = withContext(Dispatchers.IO) {
        try {
            val modelFile = File(modelPath)
            if (!modelFile.exists()) {
                Log.e(TAG, "‚ùå Mod√®le non trouv√©: $modelPath")
                return@withContext false
            }
            
            Log.d(TAG, "üîÑ Chargement mod√®le: ${modelFile.name} (${modelFile.length() / 1024 / 1024} MB)...")
            
            // Param√®tres optimis√©s
            val threads = Runtime.getRuntime().availableProcessors()
            val contextSize = 4096  // 4K tokens de contexte
            val useGPU = true  // Utiliser GPU si disponible
            
            isModelLoaded = nativeLoadModel(modelPath, threads, contextSize, useGPU)
            
            if (isModelLoaded) {
                // Initialiser la m√©moire conversationnelle
                conversationMemory = ConversationMemory(context, characterId)
                Log.i(TAG, "‚úÖ Mod√®le charg√© avec succ√®s")
                Log.d(TAG, "   ‚Üí Threads: $threads, Context: $contextSize, GPU: $useGPU")
            } else {
                Log.e(TAG, "‚ùå √âchec chargement mod√®le")
            }
            
            isModelLoaded
        } catch (e: Exception) {
            Log.e(TAG, "‚ùå Erreur chargement mod√®le", e)
            false
        }
    }
    
    /**
     * G√©n√®re une r√©ponse avec le mod√®le local
     */
    suspend fun generateResponse(
        character: Character,
        messages: List<Message>,
        username: String = "Utilisateur"
    ): String = withContext(Dispatchers.IO) {
        if (!isModelLoaded) {
            throw Exception("Mod√®le non charg√©")
        }
        
        try {
            Log.d(TAG, "===== G√©n√©ration avec LLM local =====")
            
            // Ajouter les messages √† la m√©moire
            messages.takeLast(5).forEach { msg ->
                conversationMemory?.addMessage(msg)
            }
            
            // Construire le prompt avec m√©moire
            val prompt = buildPromptWithMemory(character, messages, username)
            
            Log.d(TAG, "üì§ Prompt: ${prompt.length} caract√®res")
            Log.d(TAG, "üß† Niveau relation: ${conversationMemory?.getRelationshipLevel() ?: 0}/100")
            
            // Param√®tres de g√©n√©ration optimis√©s
            val response = nativeGenerate(
                prompt = prompt,
                maxTokens = 250,
                temperature = 0.85f,  // Cr√©atif mais coh√©rent
                topP = 0.95f,
                topK = 40,
                repeatPenalty = 1.15f,  // Anti-r√©p√©tition
                stopSequences = arrayOf("\n$username:", "\nUtilisateur:", "\n\n")
            )
            
            // Nettoyer la r√©ponse
            val cleaned = cleanResponse(response, character.name)
            
            Log.i(TAG, "‚úÖ R√©ponse g√©n√©r√©e (${cleaned.length} caract√®res)")
            
            // Ajouter la r√©ponse √† la m√©moire
            conversationMemory?.addMessage(Message(
                id = "",
                chatId = "",
                content = cleaned,
                isUser = false,
                timestamp = System.currentTimeMillis()
            ))
            
            cleaned
        } catch (e: Exception) {
            Log.e(TAG, "‚ùå Erreur g√©n√©ration", e)
            throw Exception("Erreur g√©n√©ration LLM: ${e.message}")
        }
    }
    
    /**
     * Construit le prompt avec m√©moire conversationnelle
     */
    private fun buildPromptWithMemory(
        character: Character,
        messages: List<Message>,
        username: String
    ): String {
        val prompt = StringBuilder()
        
        // Syst√®me prompt
        prompt.append(buildSystemPrompt(character, username))
        prompt.append("\n\n")
        
        // Contexte de m√©moire long terme
        conversationMemory?.let { memory ->
            val relevantContext = memory.getRelevantContext(messages)
            if (relevantContext.isNotEmpty()) {
                prompt.append(relevantContext)
                prompt.append("\n\n")
            }
        }
        
        // Conversation r√©cente (10 derniers messages)
        prompt.append("### CONVERSATION ACTUELLE ###\n")
        messages.takeLast(10).forEach { msg ->
            val speaker = if (msg.isUser) username else character.name
            prompt.append("$speaker: ${msg.content}\n")
        }
        
        // D√©but de la r√©ponse
        prompt.append("\n${character.name}:")
        
        return prompt.toString()
    }
    
    /**
     * Construit le syst√®me prompt
     */
    private fun buildSystemPrompt(character: Character, username: String): String {
        val nsfwInstructions = if (nsfwMode) {
            "\n- Tu peux d√©crire du contenu adulte de mani√®re naturelle et d√©taill√©e"
        } else {
            ""
        }
        
        // R√©cup√©rer les faits de la m√©moire
        val facts = conversationMemory?.getFacts() ?: emptyMap()
        val factsText = if (facts.isNotEmpty()) {
            "\n\n**CE QUE TU SAIS** :\n" + facts.entries.joinToString("\n") { "- ${it.key}: ${it.value}" }
        } else {
            ""
        }
        
        return """### SYST√àME ###

Tu es ${character.name}.

**PERSONNALIT√â** : ${character.personality ?: "Unique"}
**DESCRIPTION** : ${character.description ?: ""}
$factsText

**R√àGLES** :
- Reste coh√©rent avec ta personnalit√©
- Utilise (*actions*) et (pens√©es)
- Pas de m√©tadonn√©es ou annotations
- R√©ponds naturellement √† $username
- Fais √©voluer la relation progressivement$nsfwInstructions

Incarne ${character.name} de mani√®re immersive."""
    }
    
    /**
     * Nettoie la r√©ponse
     */
    private fun cleanResponse(response: String, characterName: String): String {
        var cleaned = response.trim()
        
        // Retirer les pr√©fixes
        cleaned = cleaned.replace(Regex("^$characterName\\s*:\\s*"), "")
        cleaned = cleaned.replace(Regex("^(Assistant|AI)\\s*:\\s*"), "")
        
        // Retirer m√©tadonn√©es
        cleaned = cleaned.replace(Regex("\\(OOC:.*?\\)"), "")
        cleaned = cleaned.replace(Regex("\\[.*?\\]"), "")
        
        // Couper √† la premi√®re ligne vide ou nouveau speaker
        val lines = cleaned.split("\n")
        val validLines = mutableListOf<String>()
        for (line in lines) {
            if (line.trim().isEmpty() || line.matches(Regex("^[A-Z][a-z]+\\s*:"))) {
                break
            }
            validLines.add(line)
        }
        
        return validLines.joinToString("\n").trim()
    }
    
    /**
     * D√©charge le mod√®le
     */
    fun unloadModel() {
        if (isModelLoaded) {
            nativeUnloadModel()
            isModelLoaded = false
            Log.d(TAG, "üßπ Mod√®le d√©charg√©")
        }
    }
    
    /**
     * V√©rifie si un mod√®le existe
     */
    fun isModelAvailable(): Boolean {
        return File(modelPath).exists()
    }
}
