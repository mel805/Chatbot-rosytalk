package com.roleplayai.chatbot.data.ai

import android.content.Context
import android.util.Log
import com.roleplayai.chatbot.data.model.Character
import com.roleplayai.chatbot.data.model.Message
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.withTimeout
import kotlinx.coroutines.withContext
import java.io.File
import kotlin.math.max

/**
 * Moteur llama.cpp (LOCAL) via JNI.
 *
 * IMPORTANT:
 * - Ce moteur utilise un VRAI mod√®le GGUF fourni par l'utilisateur (stock√© dans /models).
 * - La lib native est compil√©e via NDK. En CI, les sources llama.cpp sont r√©cup√©r√©es
 *   automatiquement (voir workflow) pour builder la lib.
 */
class LlamaCppEngine(private val context: Context) {
    
    companion object {
        private const val TAG = "LlamaCppEngine"

        @Volatile private var nativeLibLoaded: Boolean = false

        init {
            try {
                System.loadLibrary("llama-android")
                nativeLibLoaded = true
                Log.i(TAG, "‚úÖ Biblioth√®que native llama-android charg√©e")
            } catch (e: UnsatisfiedLinkError) {
                nativeLibLoaded = false
                Log.w(TAG, "‚ö†Ô∏è Biblioth√®que native llama-android indisponible: ${e.message}")
            } catch (e: SecurityException) {
                nativeLibLoaded = false
                Log.w(TAG, "‚ö†Ô∏è Impossible de charger llama-android: ${e.message}")
            }
        }

        // JNI (voir app/src/main/cpp/llama-android.cpp)
        @JvmStatic private external fun loadModel(modelPath: String, nThreads: Int, nCtx: Int): Long
        @JvmStatic private external fun generate(
            contextPtr: Long,
            prompt: String,
            maxTokens: Int,
            temperature: Float,
            topP: Float,
            topK: Int,
            repeatPenalty: Float
        ): String
        @JvmStatic private external fun freeModel(contextPtr: Long)
        @JvmStatic private external fun isModelLoaded(contextPtr: Long): Boolean
    }
    
    private var modelPath: String? = null
    private var contextPtr: Long = 0L
    
    fun setModelPath(path: String) {
        modelPath = path
        Log.i(TAG, "üìÅ Mod√®le configur√©: $path")
    }
    
    fun isAvailable(): Boolean {
        val path = modelPath
        return nativeLibLoaded && path != null && File(path).exists()
    }

    private fun defaultThreads(): Int {
        // Sur mobile, trop de threads peut √™tre contre-productif.
        val cpu = Runtime.getRuntime().availableProcessors()
        return max(1, minOf(4, cpu))
    }

    private fun ensureLoadedOrThrow() {
        val path = modelPath ?: throw IllegalStateException("Aucun mod√®le GGUF configur√© (Param√®tres > llama.cpp)")
        val f = File(path)
        if (!f.exists()) throw IllegalStateException("Mod√®le GGUF introuvable: $path")
        if (!nativeLibLoaded) throw IllegalStateException("Lib native llama-android indisponible sur cet appareil/build")

        if (contextPtr != 0L && isModelLoaded(contextPtr)) {
            return
        }

        // Charger / recharger le mod√®le
        if (contextPtr != 0L) {
            try {
                freeModel(contextPtr)
            } catch (e: Throwable) {
                Log.w(TAG, "‚ö†Ô∏è freeModel a √©chou√© (on continue): ${e.message}")
            } finally {
                contextPtr = 0L
            }
        }

        val threads = defaultThreads()
        val nCtx = 2048
        contextPtr = loadModel(path, threads, nCtx)
        if (contextPtr == 0L || !isModelLoaded(contextPtr)) {
            contextPtr = 0L
            throw IllegalStateException("√âchec chargement mod√®le llama.cpp (v√©rifie le GGUF et l'espace disque)")
        }
    }
    
    /**
     * G√©n√®re une r√©ponse locale (llama.cpp) coh√©rente et immersive.
     */
    suspend fun generateResponse(
        character: Character,
        messages: List<Message>,
        username: String = "Utilisateur",
        userGender: String = "neutre",
        memoryContext: String = "",
        nsfwMode: Boolean = false
    ): String = withContext(Dispatchers.IO) {
        ensureLoadedOrThrow()

        val prompt = buildPrompt(
            character = character,
            messages = messages,
            username = username,
            userGender = userGender,
            memoryContext = memoryContext,
            nsfwMode = nsfwMode
        )

        try {
            // Limites strictes pour √©viter les "r√©flexions infinies" sur appareils lents
            val raw = withTimeout(45_000) {
                generate(
                    contextPtr = contextPtr,
                    prompt = prompt,
                    maxTokens = 160,
                    temperature = 0.85f,
                    topP = 0.92f,
                    topK = 40,
                    repeatPenalty = 1.15f
                )
            }

            val cleaned = cleanModelOutput(raw, character.name)
            if (cleaned.isBlank()) {
                throw IllegalStateException("R√©ponse vide du mod√®le local")
            }
            return@withContext cleaned
        } catch (e: kotlinx.coroutines.TimeoutCancellationException) {
            // Note: l'appel natif ne peut pas √™tre interrompu proprement, on limite donc maxTokens
            // et on remonte un message clair.
            Log.e(TAG, "‚è±Ô∏è Timeout g√©n√©ration llama.cpp", e)
            throw IllegalStateException("Le mod√®le local met trop de temps √† r√©pondre. Essaie un mod√®le GGUF plus l√©ger (TinyLlama/Phi-2) ou baisse la charge.")
        } catch (e: Exception) {
            Log.e(TAG, "‚ùå Erreur g√©n√©ration llama.cpp", e)
            throw e
        }
    }

    private fun buildPrompt(
        character: Character,
        messages: List<Message>,
        username: String,
        userGender: String,
        memoryContext: String,
        nsfwMode: Boolean
    ): String {
        val recent = messages.takeLast(16)

        val nsfwLine = if (nsfwMode) {
            "- Mode adulte: reste consensuel, progression naturelle, coh√©rent avec le temp√©rament."
        } else {
            "- Contenu tout public."
        }

        val sb = StringBuilder()
        sb.append(
            """
            ### SYSTEM
            Tu es ${character.name} (personnage de roleplay), pas un assistant.
            - Personnalit√©: ${character.personality}
            - Description: ${character.description}
            - Sc√©nario: ${character.scenario}
            $nsfwLine
            - R√®gles: ne d√©cris QUE tes actions (pas celles de l'utilisateur). Reste fid√®le au caract√®re/temperament.
            - Initiative: r√©agis + fais avancer la sc√®ne (propose une action ou un angle), pose au plus une question utile.
            - Style: 1-3 paragraphes, immersif, concret, pas de m√©tadonn√©es.
            - Format: *action* (pens√©e) "dialogue"
            Utilisateur: $username (sexe: $userGender)
            """.trimIndent()
        )

        if (memoryContext.isNotBlank()) {
            sb.append("\n\n### MEMOIRE\n")
            sb.append(memoryContext.trim())
        }

        sb.append("\n\n### CONVERSATION\n")
        for (m in recent) {
            val speaker = if (m.isUser) username else character.name
            sb.append(speaker).append(": ").append(m.content.trim()).append("\n")
        }

        sb.append("\n### REPONSE\n")
        sb.append(character.name).append(":")
        return sb.toString()
    }

    private fun cleanModelOutput(raw: String, characterName: String): String {
        var out = raw.trim()
        out = out.removePrefix("$characterName:")
        out = out.trim()

        // Couper si le mod√®le recommence un nouveau speaker
        val lines = out.lines()
        val kept = mutableListOf<String>()
        for (line in lines) {
            val t = line.trim()
            if (t.matches(Regex("^(Utilisateur|$characterName|Assistant|IA)\\s*:.*", RegexOption.IGNORE_CASE))) {
                break
            }
            kept.add(line)
        }
        return kept.joinToString("\n").trim()
    }
    
    fun getAvailableModels(): List<File> {
        val modelsDir = File(context.getExternalFilesDir(null), "models")
        if (!modelsDir.exists()) {
            modelsDir.mkdirs()
            return emptyList()
        }
        
        return modelsDir.listFiles { file ->
            file.extension == "gguf"
        }?.toList() ?: emptyList()
    }
    
    fun getModelsDirectory(): File {
        val modelsDir = File(context.getExternalFilesDir(null), "models")
        if (!modelsDir.exists()) {
            modelsDir.mkdirs()
        }
        return modelsDir
    }
}
