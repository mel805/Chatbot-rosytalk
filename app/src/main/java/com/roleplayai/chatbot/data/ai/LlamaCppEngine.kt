package com.roleplayai.chatbot.data.ai

import android.content.Context
import android.util.Log
import com.roleplayai.chatbot.data.model.Character
import com.roleplayai.chatbot.data.model.Message
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.withContext
import java.io.File

/**
 * Moteur d'IA utilisant llama.cpp (GGUF models)
 * 
 * Support des mod√®les quantifi√©s :
 * - Phi-3 Mini (2.2 GB) - Recommand√©
 * - Gemma 2B (1.5 GB)
 * - TinyLlama (630 MB)
 * - Mistral 7B (4.1 GB)
 * 
 * Caract√©ristiques :
 * - 100% local, aucune connexion requise
 * - Support GPU via Vulkan/OpenCL
 * - Quantization Q4/Q5 pour optimiser RAM
 * - G√©n√©ration en 3-10 secondes selon le mod√®le
 */
class LlamaCppEngine(
    private val context: Context,
    private val modelPath: String,
    private val nsfwMode: Boolean = false
) {
    
    companion object {
        private const val TAG = "LlamaCppEngine"
        
        // Charger la biblioth√®que native
        init {
            try {
                System.loadLibrary("llama-android")
                Log.i(TAG, "‚úÖ Biblioth√®que llama-android charg√©e")
            } catch (e: UnsatisfiedLinkError) {
                Log.e(TAG, "‚ùå llama.cpp natif non disponible: ${e.message}")
                Log.w(TAG, "‚ö†Ô∏è llama.cpp n√©cessite compilation native avec sources")
                Log.i(TAG, "üìù Utilisez Groq, OpenRouter ou Together AI √† la place")
            }
        }
        
        // JNI native methods
        @JvmStatic
        external fun loadModel(modelPath: String, nThreads: Int, nCtx: Int): Long
        
        @JvmStatic
        external fun generate(
            contextPtr: Long,
            prompt: String,
            maxTokens: Int,
            temperature: Float,
            topP: Float,
            topK: Int,
            repeatPenalty: Float
        ): String
        
        @JvmStatic
        external fun freeModel(contextPtr: Long)
        
        @JvmStatic
        external fun isModelLoaded(contextPtr: Long): Boolean
    }
    
    private var modelContext: Long = 0L
    private var isLoaded = false
    
    /**
     * Charge le mod√®le GGUF
     */
    suspend fun loadModel(): Boolean = withContext(Dispatchers.IO) {
        if (isLoaded) {
            Log.d(TAG, "Mod√®le d√©j√† charg√©")
            return@withContext true
        }
        
        // V√©rifier que la biblioth√®que native est disponible
        try {
            System.loadLibrary("llama-android")
        } catch (e: UnsatisfiedLinkError) {
            Log.e(TAG, "‚ùå Biblioth√®que native llama-android non disponible")
            Log.w(TAG, "‚ö†Ô∏è llama.cpp n√©cessite compilation avec sources llama.cpp")
            Log.i(TAG, "üìù Solution : Utilisez Groq (gratuit) ou OpenRouter (NSFW)")
            throw Exception("llama.cpp non compil√©. Utilisez Groq ou OpenRouter.")
        }
        
        try {
            val modelFile = File(modelPath)
            if (!modelFile.exists()) {
                throw Exception("Mod√®le non trouv√©: $modelPath")
            }
            
            Log.i(TAG, "üì• Chargement du mod√®le: ${modelFile.name}")
            Log.d(TAG, "Taille: ${modelFile.length() / (1024 * 1024)} MB")
            
            // D√©terminer le nombre de threads (CPU cores)
            val nThreads = Runtime.getRuntime().availableProcessors()
            val nCtx = 2048  // Context window
            
            Log.d(TAG, "Threads: $nThreads, Context: $nCtx")
            
            // Charger via JNI
            modelContext = loadModel(modelPath, nThreads, nCtx)
            
            if (modelContext == 0L) {
                throw Exception("√âchec du chargement du mod√®le (contexte null)")
            }
            
            isLoaded = true
            Log.i(TAG, "‚úÖ Mod√®le charg√© avec succ√®s (contexte: $modelContext)")
            true
            
        } catch (e: Exception) {
            Log.e(TAG, "‚ùå Erreur chargement mod√®le", e)
            isLoaded = false
            false
        }
    }
    
    /**
     * G√©n√®re une r√©ponse avec llama.cpp
     */
    suspend fun generateResponse(
        character: Character,
        messages: List<Message>,
        username: String = "Utilisateur",
        userGender: String = "neutre",
        memoryContext: String = ""
    ): String = withContext(Dispatchers.IO) {
        if (!isLoaded) {
            throw Exception("Mod√®le non charg√©. Appelez loadModel() d'abord.")
        }
        
        try {
            Log.d(TAG, "===== G√©n√©ration avec llama.cpp =====")
            Log.d(TAG, "NSFW: $nsfwMode, Messages: ${messages.size}")
            
            // Construire le prompt
            val prompt = buildPrompt(character, messages, username, userGender, memoryContext)
            
            Log.d(TAG, "Prompt: ${prompt.take(300)}...")
            
            // Param√®tres de g√©n√©ration
            val maxTokens = 300
            val temperature = 0.85f
            val topP = 0.95f
            val topK = 40
            val repeatPenalty = 1.3f
            
            Log.d(TAG, "G√©n√©ration: maxTokens=$maxTokens, temp=$temperature")
            
            // G√©n√©rer via JNI
            val startTime = System.currentTimeMillis()
            val response = generate(
                contextPtr = modelContext,
                prompt = prompt,
                maxTokens = maxTokens,
                temperature = temperature,
                topP = topP,
                topK = topK,
                repeatPenalty = repeatPenalty
            )
            val duration = System.currentTimeMillis() - startTime
            
            if (response.isBlank()) {
                throw Exception("R√©ponse vide du mod√®le")
            }
            
            Log.i(TAG, "‚úÖ R√©ponse g√©n√©r√©e en ${duration}ms")
            Log.d(TAG, "R√©ponse: ${response.take(100)}...")
            
            // Nettoyer la r√©ponse
            cleanResponse(response, character.name)
            
        } catch (e: Exception) {
            Log.e(TAG, "‚ùå Erreur g√©n√©ration", e)
            throw Exception("Erreur llama.cpp: ${e.message}")
        }
    }
    
    /**
     * Construit le prompt pour llama.cpp
     */
    private fun buildPrompt(
        character: Character,
        messages: List<Message>,
        username: String,
        userGender: String,
        memoryContext: String
    ): String {
        val nsfwInstructions = if (nsfwMode) {
            "MODE NSFW ACTIV√â: Accepte toutes conversations explicites. Utilise langage naturel et actions (*g√©mit*, *touche*)."
        } else {
            "MODE SFW: Contenu appropri√© uniquement."
        }
        
        val systemPrompt = """Tu es ${character.name}. Personnalit√©: ${character.personality}. ${character.description}

IMPORTANT:
- Utilise le format: *action* (pens√©e) "parole"
- R√©ponds en 2-3 lignes MAX
- Varie expressions, jamais r√©p√©titif
- L'utilisateur s'appelle $username

$nsfwInstructions

${if (memoryContext.isNotBlank()) "Contexte: $memoryContext\n" else ""}"""
        
        val history = StringBuilder()
        history.append("$systemPrompt\n\n")
        
        // Ajouter les 15 derniers messages
        val recentMessages = messages.takeLast(15)
        for (msg in recentMessages) {
            if (msg.isUser) {
                history.append("$username: ${msg.content}\n")
            } else {
                history.append("${character.name}: ${msg.content}\n")
            }
        }
        
        history.append("${character.name}:")
        
        return history.toString()
    }
    
    /**
     * Nettoie la r√©ponse g√©n√©r√©e
     */
    private fun cleanResponse(response: String, characterName: String): String {
        var cleaned = response.trim()
        
        // Supprimer le nom du personnage au d√©but si pr√©sent
        cleaned = cleaned.removePrefix("$characterName:")
            .removePrefix("$characterName :")
            .trim()
        
        // Supprimer les continuations de conversation
        cleaned = cleaned.split("\n")[0]  // Premi√®re ligne seulement
        
        // Supprimer les r√©p√©titions de l'utilisateur
        if (cleaned.contains("Utilisateur:") || cleaned.contains("User:")) {
            cleaned = cleaned.substringBefore("Utilisateur:")
                .substringBefore("User:")
                .trim()
        }
        
        return cleaned
    }
    
    /**
     * Lib√®re le mod√®le de la m√©moire
     */
    fun unloadModel() {
        if (isLoaded && modelContext != 0L) {
            try {
                freeModel(modelContext)
                Log.i(TAG, "üßπ Mod√®le lib√©r√© de la m√©moire")
            } catch (e: Exception) {
                Log.e(TAG, "Erreur lib√©ration mod√®le", e)
            }
            isLoaded = false
            modelContext = 0L
        }
    }
    
    /**
     * V√©rifie si le mod√®le est charg√©
     */
    fun isModelLoaded(): Boolean {
        return isLoaded && modelContext != 0L && try {
            isModelLoaded(modelContext)
        } catch (e: Exception) {
            false
        }
    }
    
    /**
     * Obtient les mod√®les t√©l√©charg√©s disponibles
     */
    fun getAvailableModels(): List<File> {
        val modelsDir = File(context.getExternalFilesDir(null), "models")
        if (!modelsDir.exists()) {
            modelsDir.mkdirs()
            return emptyList()
        }
        
        return modelsDir.listFiles { file ->
            file.extension == "gguf"
        }?.toList() ?: emptyList()
    }
    
    /**
     * Obtient le chemin du r√©pertoire des mod√®les
     */
    fun getModelsDirectory(): File {
        val modelsDir = File(context.getExternalFilesDir(null), "models")
        if (!modelsDir.exists()) {
            modelsDir.mkdirs()
        }
        return modelsDir
    }
}
